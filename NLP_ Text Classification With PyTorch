{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPM31cebqYeKzjziSaSGxCk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gi1a6w4_BQ7v","executionInfo":{"status":"ok","timestamp":1698206333812,"user_tz":240,"elapsed":17867,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"fd69e017-254c-4971-80a6-f62c16644126"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}],"source":["!pip install portalocker\n","!pip install torchmetrics"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.datasets import DATASETS\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.utils import get_tokenizer\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import numpy\n","import random"],"metadata":{"id":"mCgGBeL6BXv9","executionInfo":{"status":"ok","timestamp":1698207358598,"user_tz":240,"elapsed":150,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["DATASET = \"IMDB\"\n","DATA_DIR = \".data\"\n","BATCH_SIZE = 2 # Very small, you'll see why\n","# Set the device to use for training\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"1S5oR-M9td49","executionInfo":{"status":"ok","timestamp":1698207062727,"user_tz":240,"elapsed":141,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["TOKENIZER = get_tokenizer(\"basic_english\")"],"metadata":{"id":"5XDQZdZUtkr-","executionInfo":{"status":"ok","timestamp":1698207066408,"user_tz":240,"elapsed":131,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["TOKENIZER(\"This is a sentence ahsahshas ahshahsh\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFKzf-q75uNu","executionInfo":{"status":"ok","timestamp":1698207067422,"user_tz":240,"elapsed":3,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"0be2c5c4-16a0-4731-f857-a4964a325851"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this', 'is', 'a', 'sentence', 'ahsahshas', 'ahshahsh']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Load the IMDB dataset and split it into training and test sets\n","# Broken? torchtext is good to use sometimes but it is not maintained ...\n","_, train_iter = DATASETS[DATASET](root=DATA_DIR)"],"metadata":{"id":"HgPsrq6KurRo","executionInfo":{"status":"ok","timestamp":1698207069308,"user_tz":240,"elapsed":169,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# What is inside of this data?\n","labels = {}\n","for i, (label, text) in enumerate(train_iter):\n","  if label not in labels:\n","    labels[label] = 0\n","    print(label, text)\n","  labels[label] += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pztmFCK2uvYk","executionInfo":{"status":"ok","timestamp":1698206509931,"user_tz":240,"elapsed":1106,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"78dcdc31-d43c-43e9-e631-e4426d07a895"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1 I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n","2 Previous reviewer Claudio Carvalho gave a much better recap of the film's plot details than I could. What I recall mostly is that it was just so beautiful, in every sense - emotionally, visually, editorially - just gorgeous.<br /><br />If you like movies that are wonderful to look at, and also have emotional content to which that beauty is relevant, I think you will be glad to have seen this extraordinary and unusual work of art.<br /><br />On a scale of 1 to 10, I'd give it about an 8.75. The only reason I shy away from 9 is that it is a mood piece. If you are in the mood for a really artistic, very romantic film, then it's a 10. I definitely think it's a must-see, but none of us can be in that mood all the time, so, overall, 8.75.\n"]}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1SpkDo9_7Or","executionInfo":{"status":"ok","timestamp":1698206564252,"user_tz":240,"elapsed":139,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"7fd11405-4c08-44d2-8075-4c38c721ad56"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{1: 12500, 2: 12500}"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Build the vocabulary from the training data\n","# This is basically a map going from a string to a unique integer\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield TOKENIZER(text)\n","\n","# Get a dictionary of words; rare words are mapped to to the <unk> token\n","VOCAB = build_vocab_from_iterator(\n","    yield_tokens(train_iter),\n","    specials=('<pad>', '<unk>'),\n","    min_freq = 15\n",")\n","\n","# Make the default index the same as that of the unk_token.\n","VOCAB.set_default_index(VOCAB['<unk>'])\n","\n","print(len(VOCAB))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obEDeJDI6sbe","executionInfo":{"status":"ok","timestamp":1698207081563,"user_tz":240,"elapsed":7239,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"67a82d52-1ff3-46ae-ffa5-a32ef4a24c9a"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["15642\n"]}]},{"cell_type":"code","source":["def text_pipeline(text):\n","    return VOCAB(TOKENIZER(text))\n","\n","def label_pipeline(label):\n","    # Have to map the labels {1, 2} to {0, 1}\n","    return int(label) -1"],"metadata":{"id":"Po1xdtX236_L","executionInfo":{"status":"ok","timestamp":1698207084904,"user_tz":240,"elapsed":118,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# What does this do?\n","# For each batch, put it in the right format\n","def collate_batch(batch):\n","    label_list, text_list = [], []\n","    for (_label, _text) in batch:\n","        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3}\n","        label_list.append(label_pipeline(_label))\n","\n","        # Return a list of ints.\n","        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n","        text_list.append(processed_text.clone().detach())\n","\n","    label_list = torch.tensor(label_list, dtype=torch.int64)\n","\n","    # Pad everything so it is the same dimension\n","    text_list = pad_sequence(text_list, batch_first=True)\n","\n","    return label_list.to(DEVICE), text_list.to(DEVICE)"],"metadata":{"id":"ehkH8hB037Hc","executionInfo":{"status":"ok","timestamp":1698206819394,"user_tz":240,"elapsed":135,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["train_ds = to_map_style_dataset(train_iter)"],"metadata":{"id":"ZEQhh5Ud7F19","executionInfo":{"status":"ok","timestamp":1698207091046,"user_tz":240,"elapsed":790,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"],"metadata":{"id":"SiKT0OyM5mGC","executionInfo":{"status":"ok","timestamp":1698207092164,"user_tz":240,"elapsed":149,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["for yb, xb in train_dl:\n","    print(yb.shape, xb.shape)\n","    print(xb)\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RliS2dltPGz","executionInfo":{"status":"ok","timestamp":1698207122786,"user_tz":240,"elapsed":4,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"9dfb1008-7b79-4ee0-c434-0a38b44ee210"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2]) torch.Size([2, 156])\n","tensor([[   13,   290,    14,    19,    12,  8692,     5,    14,    19,    17,\n","             6,   245,     7,   829,   339,     8,    76,    50,    93,   648,\n","            10,     8,   467,     2,     1,     5,     2,  1402,   129,     3,\n","            59,    13,   203,     6,   726,    12, 15062,     4,    13,   468,\n","            95,     1,    26,    12,    19,     3,     3,     1,     9,    16,\n","           726,    24,     8,   118,   798,    14,    19,     3,    40,     2,\n","           104,     4,    13,    33,     6,   872,    14,    19,     4,    22,\n","            10,    12,  8051,  1076,     5,     2,   487,    10,    29,    44,\n","            58,    45,     6,   260,    47,     6,   183,  1746,    26,    13,\n","          3753,   219,   908,   165,   600,     5,    12,     1,     1,     3,\n","             3,    44,     2,   487,    10,    29,    44,    58,     3,     3,\n","            24,    49,  1753,   622,     6,   872,     3,     3,    13,     9,\n","           238,   358, 11103,     8,   260,     5,    13,    57,  2282,    21,\n","            25,   558,    53,     1,     9,    16,     6,    86,    19,     5,\n","            13,   986,    15,    10,     6,    19,     8,    34,   221,   290,\n","             3,     1,     1,    82,     1,     1],\n","        [   13,     9,   149,  8197,     2,   314,   807,    33,   664,  7433,\n","            82,   903,    38,     2,  1204,     1,     4,    13,    17,  1804,\n","             8,  3005,    15,    63,  1637,     7,   106,    26,     5,   129,\n","            24,    17,   333,   288,    81,   294,   328,     9,    16,    12,\n","             2,   187,     3,   263,    12,    14,    19, 14356,  2709,     8,\n","            76,     3,     3,     3,     2,   110,     4,     2,   412,     4,\n","             2,  5291,   682,     4,     2,  2145,  2713,     4,    38,     7,\n","            11,     3,    18,   253,   136, 13036,    13,   763,   538,     4,\n","            90,    94,     9,    28,   500,    45,    14,     3,    11,     9,\n","            16,  9363,  7168,     5, 11791,     3,    12,     6,   104,    13,\n","           111,     2,    19,  9237,    25,    92,  3464,    11,     4,    47,\n","          4272,     8,    45,    11,     4,    96,    11,     9,    16,   571,\n","             5,    50,   158,    90,     5,  7605,  1165,     3,    22,   134,\n","             4,    11,    71,   121,    29,   174,    21,    76,     3,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0]])\n"]}]},{"cell_type":"code","source":["# Question: What is this doing? This is very bad\n","# For data of size (N, T), get the one hot for each word to get (N, T, V)\n","xb = nn.functional.one_hot(xb, num_classes=len(VOCAB))\n","print(xb.shape)\n","# Add up across the T words to get the sentence representation (N, V)\n","xb = xb.sum(axis=1)\n","print(xb.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIZfSP-N9zEO","executionInfo":{"status":"ok","timestamp":1698207127154,"user_tz":240,"elapsed":186,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}},"outputId":"09af2a05-41ec-469f-f521-50d5c5f82ec4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 156, 15642])\n","torch.Size([2, 15642])\n"]}]},{"cell_type":"code","source":["# Define the logistic regression model\n","class LogisticRegression(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LogisticRegression, self).__init__()\n","        # This is like a linear layer with\n","        self.linear = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"metadata":{"id":"orY_z36C2I2z","executionInfo":{"status":"ok","timestamp":1698207147507,"user_tz":240,"elapsed":141,"user":{"displayName":"Andrei Simion","userId":"18320599517202851288"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Set the hyperparameters\n","input_dim = len(VOCAB)\n","output_dim = 2\n","lr = 0.00001\n","epochs = 10\n","skip_probability = 0.95\n","\n","# Initialize the model and optimizer\n","model = LogisticRegression(input_dim, output_dim).to(DEVICE)\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","# Train the model\n","for epoch in range(epochs):\n","    for yb, xb in tqdm(train_dl):\n","        # Effectively use just 5% of the train data\n","        if random.random() < skip_probability:\n","          continue\n","        optimizer.zero_grad()\n","        xb = xb.to(DEVICE)\n","\n","        xb = nn.functional.one_hot(xb, num_classes=len(VOCAB))\n","        # Add up across the T words to get the sentence representation (N, V)\n","        xb = xb.sum(axis=1)\n","\n","        yb = yb.to(DEVICE)\n","        y_pred = model(xb.float())\n","        loss = nn.CrossEntropyLoss()(y_pred, yb)\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate the model on the test set\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for yb, xb in tqdm(train_dl):\n","            # Effectively use just 5% of the train data\n","            if random.random() < skip_probability:\n","              continue\n","            xb = xb.to(DEVICE)\n","\n","            xb = nn.functional.one_hot(xb, num_classes=len(VOCAB))\n","            # Add up across the T words to get the sentence representation (N, V)\n","            xb = xb.sum(axis=1)\n","\n","            yb = yb.to(DEVICE)\n","            y_pred = model(xb.float())\n","            _, predicted = torch.max(y_pred.data, 1)\n","            total += yb.size(0)\n","            correct += (predicted == yb).sum().item()\n","        accuracy = 100 * correct / total\n","        print('Epoch: {}, Train Accuracy: {}%'.format(epoch+1, accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOfA1YMauhTD","outputId":"c62e23ad-a3f0-419a-dc2a-38b887356260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12500/12500 [00:58<00:00, 213.05it/s]\n","100%|██████████| 12500/12500 [00:54<00:00, 229.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, Train Accuracy: 50.25423728813559%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12500/12500 [00:56<00:00, 220.85it/s]\n","100%|██████████| 12500/12500 [00:50<00:00, 246.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 2, Train Accuracy: 60.11904761904762%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12500/12500 [00:55<00:00, 226.39it/s]\n","100%|██████████| 12500/12500 [00:54<00:00, 229.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 3, Train Accuracy: 62.26890756302521%\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12500/12500 [00:55<00:00, 227.11it/s]\n","100%|██████████| 12500/12500 [00:53<00:00, 235.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 4, Train Accuracy: 70.1827242524917%\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▉        | 2395/12500 [00:10<01:14, 135.08it/s]"]}]},{"cell_type":"markdown","source":["Why am I not making the batch size larger?\n","\n","Why is this a bad model?"],"metadata":{"id":"Tw6c05s-CY3l"}}]}